{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><sub>This notebook is distributed under the <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\" target=\"_blank\">Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license</a>.</sub></div>\n",
    "<h1>Hands on Machine Learning  <span style=\"font-size:12px;\"><i>by <a href=\"https://webgrec.ub.edu/webpages/tmp/cat/dmaluenda.ub.edu.html\" target=\"_blank\">David Maluenda</a></i></span></h1>\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=71605\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/02_Training.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/02_Training.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/02_Training.ipynb\"  target=\"_blank\"\n",
    "          download=\"02_Training\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{II}$. Training Neural Networks with Pure Python\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/en/uab-official-masters-degrees-study-guides/) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "Tutorial 2\n",
    "\n",
    "This notebook shows how to:\n",
    "- implement a stochastic gradient descent to fit an arbitrary function\n",
    "- implement backpropagation in pure python\n",
    "- train a deep fully connected net to reproduce an image\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [Machine Learning for Physicists](https://machine-learning-for-physicists.org/) by Florian Marquardt.<br>\n",
    "[2] [NumPy](https://numpy.org/doc/stable/user/whatisnumpy.html): the fundamental package for scientific computing in Python.<br>\n",
    "[3] [Matplotlib](https://matplotlib.org/stable/tutorials/introductory/usage.html): a comprehensive library for creating static, animated, and interactive visualizations in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports:-only-numpy-and-matplotlib\" data-toc-modified-id=\"Imports:-only-numpy-and-matplotlib-0\"><span class=\"toc-item-num\">0&nbsp;&nbsp;</span>Imports: only numpy and matplotlib</a></span></li><li><span><a href=\"#Curve-fitting-(base-for-training)\" data-toc-modified-id=\"Curve-fitting-(base-for-training)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Curve fitting (base for training)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Function-definition-and-fitting-model\" data-toc-modified-id=\"Function-definition-and-fitting-model-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Function definition and fitting model</a></span></li><li><span><a href=\"#Cost-function\" data-toc-modified-id=\"Cost-function-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Cost function</a></span></li><li><span><a href=\"#Stochastic-gradient-descent\" data-toc-modified-id=\"Stochastic-gradient-descent-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Stochastic gradient descent</a></span></li><li><span><a href=\"#[HOMEWORK]:-Your-own-examples\" data-toc-modified-id=\"[HOMEWORK]:-Your-own-examples-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>[HOMEWORK]: Your own examples</a></span></li></ul></li><li><span><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Backpropagation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Implement-backpropagation-for-a-general-(fully-connected)-network\" data-toc-modified-id=\"Implement-backpropagation-for-a-general-(fully-connected)-network-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Implement backpropagation for a general (fully connected) network</a></span></li><li><span><a href=\"#Setup-for-a-particular-set-of-layer-sizes\" data-toc-modified-id=\"Setup-for-a-particular-set-of-layer-sizes-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Setup for a particular set of layer sizes</a></span></li><li><span><a href=\"#Train-the-net-on-one-single-batch-repeatedly-(not-so-good)\" data-toc-modified-id=\"Train-the-net-on-one-single-batch-repeatedly-(not-so-good)-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Train the net on one single batch repeatedly (not so good)</a></span></li><li><span><a href=\"#Produce-random-batches:-randomly-sample-a-function-defined-on-a-2D-square\" data-toc-modified-id=\"Produce-random-batches:-randomly-sample-a-function-defined-on-a-2D-square-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Produce random batches: randomly sample a function defined on a 2D square</a></span></li></ul></li><li><span><a href=\"#Train-net-to-reproduce-an-image\" data-toc-modified-id=\"Train-net-to-reproduce-an-image-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train net to reproduce an image</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlT_ZrS-R1rG"
   },
   "source": [
    "## Imports: only numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T15:54:31.308199Z",
     "iopub.status.busy": "2021-04-19T15:54:31.307840Z",
     "iopub.status.idle": "2021-04-19T15:54:31.896606Z",
     "shell.execute_reply": "2021-04-19T15:54:31.895654Z",
     "shell.execute_reply.started": "2021-04-19T15:54:31.308135Z"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1626874792493,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "zRmzXX4wR1rG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the \"numpy\" library for linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# get \"matplotlib\" for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 300  # highres display\n",
    "from matplotlib.axes._axes import _log as mpl_ax_logger\n",
    "mpl_ax_logger.setLevel('ERROR')  # ignore warnings\n",
    "\n",
    "\n",
    "# for nice inset colorbars:\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# time control to count it and manage it\n",
    "from time import time, sleep\n",
    "\n",
    "# For simple animation\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve fitting (base for training)\n",
    "\n",
    "This section shows how stochastic gradient descent can help to fit an arbitrary function (neural networks essentially do that, but in much higher dimensions and with many more parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definition and fitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a non linear function to fit.\n",
    "\n",
    "For instance\n",
    "\n",
    "$$f(x) = \\frac{\\theta_0}{(x - \\theta_1)^2 + 1}$$\n",
    "\n",
    "where $\\theta_0$ and $\\theta_1$ are free parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f(theta, x):\n",
    "    \"\"\" theta are the parameters\n",
    "        x are the input values (can be an array)\n",
    "    \"\"\"\n",
    "    return theta[0] / ( (x-theta[1])**2 + 1.0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a certain target functions by selecting some values fot $\\theta_0$ amd $\\theta_1$. This two values are what we want to estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_thetas = [3.0, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our fitting by taking arbitrary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_thetas = [0.5, 1.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to get randomly sampled $x$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def samples(nsamples, width=2.0):\n",
    "    \"\"\" return an array of nsamples values mostly from -width to width. \"\"\"\n",
    "    return width * np.random.randn(nsamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate the parametrized function at some sampled points to compare against actual function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = samples(100)\n",
    "\n",
    "def plot_curves(x, ax=None):\n",
    "    ax = plt.gca() if ax==None else ax\n",
    "    \n",
    "    x_line = np.linspace(-4, 4, 300)\n",
    "    ax.plot(x_line, f(true_thetas, x_line),color=\"blue\")\n",
    "    ax.plot(x_line, f(fit_thetas, x_line),color=\"orange\")\n",
    "    \n",
    "    true_points = f(true_thetas, x)\n",
    "    fit_points = f(fit_thetas, x)\n",
    "    \n",
    "    for idx in range(len(x)):\n",
    "        ax.plot([x[idx], x[idx]], [true_points[idx], fit_points[idx]], \n",
    "                'y:', linewidth=1)\n",
    "    \n",
    "    ax.scatter(x, true_points, color=\"blue\")\n",
    "    ax.scatter(x, fit_points, color=\"orange\")\n",
    "    \n",
    "    ax.legend(['ground truth', 'fitting', 'deviation'])\n",
    "    ax.set_xlim(-4,4)\n",
    "    ax.set_ylim(0.0,4.0)\n",
    "    ax.set_xlabel(r\"$x$\")\n",
    "    ax.set_ylabel(r\"$f$\") \n",
    "\n",
    "plot_curves(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "We have to know how good or bad is our fitting by defining a cost function.  We will use the average cost function of 2 parameters\n",
    "\n",
    "$$C(\\theta_0, \\theta_1) = \\frac{1}{2} |\\langle f_{\\theta_0, \\theta_1}(x) - f_{\\rm truth}(x) \\rangle|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_avg_cost(theta0s, theta1s, nsamples):\n",
    "    cost = np.zeros([len(theta0s), len(theta1s)])\n",
    "    for j0, th0 in enumerate(theta0s):\n",
    "        for j1, th1 in enumerate(theta1s):\n",
    "            x = samples(nsamples)\n",
    "            f_fit = f([th0, th1], x)\n",
    "            f_true = f(true_thetas, x)\n",
    "            cost[j0, j1] = 0.5 * np.average(( f_fit - f_true )**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the average cost function for a landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta0s = np.linspace(-3, 6, 40)  # exploring from -3 to 6 with 40 samples for th0\n",
    "theta1s = np.linspace(-2, 3, 40)  # exploring from -2 to 3 with 40 samples for th1\n",
    "\n",
    "cost = get_avg_cost(theta0s, theta1s, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this cost can be plotted with `imshow`, but let's do it more fancy. For instance with the contour function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_landscape(ax=None):\n",
    "    ax = plt.gca() if ax==None else ax\n",
    "    nlevels = 20  # number of isolines to be plotted\n",
    "    X,Y = np.meshgrid(theta0s, theta1s, indexing='ij')\n",
    "\n",
    "    im = ax.contourf(X, Y, cost, nlevels)\n",
    "    plt.colorbar(im)\n",
    "    ax.contour(X, Y, cost, nlevels, colors=\"white\")  # to highlight the isolines\n",
    "    ax.set_xlabel(r'$\\theta_0$')\n",
    "    ax.set_ylabel(r'$\\theta_1$')\n",
    "   \n",
    "plot_landscape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "We need to compute the gradient of that function. The gradient is a vector which components\n",
    "are the partial derivatives respect to each parameter.\n",
    "\n",
    "$$\\vec{\\nabla} f(x) = \\left( \\frac{\\partial f(x)}{\\partial \\, \\theta_0} \\; , \\;  \\dots \\; , \\; \\frac{\\partial f(x)}{\\partial \\, \\theta_i} \\; , \\; \\dots \\; , \\; \\frac{\\partial f(x)}{\\partial \\, \\theta_N} \\right)$$\n",
    "\n",
    "where $f(x)$ depends on different parameters $\\theta_i$ ; ($i=0\\dots N$).\n",
    "\n",
    "In this particular case\n",
    "\n",
    "$$\\vec{\\nabla} f(x) = \\left( \\frac{1}{(x - \\theta_1)^2 + 1} \\; , \\; \\frac{+2(x-\\theta_1)\\theta_0}{\\left[(x-\\theta_1)^2+1\\right]^2}  \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f_grad(thetas, x):\n",
    "    \"\"\" thetas is an array containing the value of the parameters\n",
    "        x is an array of samples where to evaluate the gradient\n",
    "    \n",
    "        Return the gradient of f with respect to theta parameters.\n",
    "        This is a vector of length equal to the number of\n",
    "        thetas parameters and evaluated on the x samples,\n",
    "        resulting on a matrix of shape (n_thetas, n_samples)\n",
    "    \"\"\"\n",
    "    df_dt0 = 1./ ( (x-thetas[1])**2 + 1.0)  # partial derivative respect to theta0\n",
    "    df_dt1 = 2 * (x-thetas[1]) * thetas[0] / ( (x-thetas[1])**2 + 1.0 )**2  # to th1\n",
    "    \n",
    "    return np.array([df_dt0, df_dt1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a gradient descent and, for each step, plot the (sampled) true function vs. the fitting function.\n",
    "Also plot the current location of parameters theta on the landscape based on the averaged cost function.\n",
    "\n",
    "Remmember that one step on the stochastic gradient descent is\n",
    "\n",
    "$$\\omega_i = \\omega_i - \\eta \\, \\hat{C} \\, \\frac{\\partial f(x)}{\\partial \\omega_i} \\quad ; \\quad (i=0,1)$$\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# functions to clean output and be able to update the display (simple animation)\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# do many steps of stochastic gradient descent,\n",
    "# continue showing the comparison!\n",
    "eta = .3  # 0.3  # \"learning rate\" (gradient descent step size)\n",
    "nsamples = 10   # stochastic x samples used per step\n",
    "nsteps = 100    # steps to do, it can be replaced with a validation criteria\n",
    "time_step = 0.01 # the updating time for animation\n",
    "\n",
    "for n in range(nsteps):  # it can be replaced with a while with some criteria\n",
    "\n",
    "    x = samples(nsamples)  # get random samples (training set)\n",
    "    \n",
    "    deviation = f(fit_thetas, x) - f(true_thetas, x)  # cost for every x (vector)\n",
    "    deviation = deviation[None, :]  # from row-vector to column-vector\n",
    "    \n",
    "    gradient = f_grad(fit_thetas, x)  # gradient for every x: shape=(2,nsamples)\n",
    "    \n",
    "    descend = np.average(deviation*gradient, axis=1)  # averaging for all x\n",
    "    \n",
    "    # do one gradient descent step:\n",
    "    fit_thetas -= eta * descend\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(8,2))\n",
    "    \n",
    "    plot_curves(x, ax[0])\n",
    "    plot_landscape(ax[1])\n",
    "\n",
    "    ax[1].scatter([fit_thetas[0]], [fit_thetas[1]], color=\"orange\")   \n",
    "    ax[1].arrow(*fit_thetas,*(-eta*descend), color='yellow',\n",
    "                width=0.05, length_includes_head=True)\n",
    "    \n",
    "    plt.show()\n",
    "#     sleep(time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [HOMEWORK]: Your own examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider for example a parametrized function\n",
    "\n",
    "      np.sin(theta[1]*(x-theta[0]))/(10.0+x**2)\n",
    "\n",
    "and a true function (in the shape of a wavepacket)\n",
    "\n",
    "      np.sin(3.0*(x-1.5))/(10.0+x**2)\n",
    "\n",
    "1. Plot and understand the function\n",
    "\n",
    "2. Plot and understand the cost function\n",
    "\n",
    "3. Run the fitting (find suitable values for eta etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement backpropagation for a general (fully connected) network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function which returns the activation and its derivative. This will be useful to do both things at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def net_f_df(z):  # calculate f(z) and f'(z)\n",
    "    val = 1/(1+np.exp(-z))  # sigmoid\n",
    "    return val, np.exp(-z)*(val**2)  # return both f and f'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a forward step will return the values after a layer and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward_step(y, w, b):  # calculate values in next layer, from input y\n",
    "    z = np.dot(y, w) + b  # w=weights, b=bias vector for next layer\n",
    "    return net_f_df(z)  # apply nonlinearity and return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_net_df(y_in, weights, biases, n_layers):  # one forward pass through the network\n",
    "\n",
    "    global y_layer, df_layer # for storing y-values and df/dz values\n",
    "    \n",
    "    y = y_in  # start with input values\n",
    "    y_layer[0] = y\n",
    "    for j in range(n_layers):  # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y, df = forward_step(y, weights[j], biases[j])  # one step\n",
    "        df_layer[j] = df  # store f'(z) [needed later in backprop]\n",
    "        y_layer[j+1] = y  # store f(z) [also needed in backprop]        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_net_simple(y_in, weights, biases, n_layers):  # one forward pass through the network\n",
    "    # no storage for backprop (this is used for simple tests)\n",
    "\n",
    "    y = y_in  # start with input values\n",
    "#     y_layer[0] = y\n",
    "    for j in range(n_layers):  # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y, df = forward_step(y, weights[j], biases[j]) # one step\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backward_step(delta, w, df): \n",
    "    # delta at layer N, of batchsize x layersize(N))\n",
    "    # w between N-1 and N [layersize(N-1) x layersize(N) matrix]\n",
    "    # df = df/dz at layer N-1, of batchsize x layersize(N-1)\n",
    "    return np.dot(delta, np.transpose(w))*df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial C}{\\partial \\omega} = $$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b} = $$\n",
    "\n",
    "$$\\Delta = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backprop(y_target, weights, biases, n_layers, batchsize): # one backward pass through the network\n",
    "    # the result will be the 'dw_layer' matrices that contain\n",
    "    # the derivatives of the cost function with respect to\n",
    "    # the corresponding weight\n",
    "    global y_layer, df_layer\n",
    "    global dw_layer, db_layer # dCost/dw and dCost/db (w,b=weights,biases)\n",
    "    \n",
    "    delta = (y_layer[-1]-y_target) * df_layer[-1]\n",
    "    dw_layer[-1] = np.dot(np.transpose(y_layer[-2]), delta) / batchsize\n",
    "    db_layer[-1] = delta.sum(0) / batchsize\n",
    "    \n",
    "    for j in range(n_layers-1):\n",
    "        delta = backward_step(delta, weights[-1-j], df_layer[-2-j])\n",
    "        dw_layer[-2-j] = np.dot(np.transpose(y_layer[-3-j]), delta)\n",
    "        db_layer[-2-j] = delta.sum(0) / batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient_step(eta, weights, biases, n_layers):  # update weights & biases (after backprop!)\n",
    "    \n",
    "    for j in range(n_layers):\n",
    "        weights[j] -= eta * dw_layer[j]\n",
    "        biases[j] -= eta * db_layer[j]\n",
    "        \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_net(y_in, y_target, eta, weights, biases, n_layers, batchsize): # one full training batch\n",
    "    # y_in is an array of size batchsize x (input-layer-size)\n",
    "    # y_target is an array of size batchsize x (output-layer-size)\n",
    "    # eta is the stepsize for the gradient descent\n",
    "    \n",
    "    y_out_result = apply_net_df(y_in, weights, biases, n_layers)\n",
    "    \n",
    "    backprop(y_target, weights, biases, n_layers, batchsize)\n",
    "    weights, biases = gradient_step(eta, weights, biases, n_layers)\n",
    "    \n",
    "    cost = ((y_target-y_out_result)**2).sum() / batchsize\n",
    "    \n",
    "    return cost, y_out_result, weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for a particular set of layer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up all the weights and biases\n",
    "\n",
    "numLayers = 3  # does not count input-layer (but does count output)\n",
    "layerSizes = [2,20,30,1]  # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "# initialize random weights and biases for all layers (except input of course)\n",
    "Weights = [np.random.uniform(low=-1, high=+1, size=[ layerSizes[j], layerSizes[j+1] ]) for j in range(numLayers)]\n",
    "Biases = [np.random.uniform(low=-1, high=+1, size=layerSizes[j+1]) for j in range(numLayers)]\n",
    "\n",
    "# define the batchsize\n",
    "batchsize=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up all the helper variables\n",
    "\n",
    "y_layer = [np.zeros([batchsize, layerSizes[j]]) for j in range(numLayers+1)]\n",
    "df_layer = [np.zeros([batchsize, layerSizes[j+1]]) for j in range(numLayers)]\n",
    "dw_layer = [np.zeros([layerSizes[j], layerSizes[j+1]]) for j in range(numLayers)]\n",
    "db_layer = [np.zeros(layerSizes[j+1]) for j in range(numLayers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the net on one single batch repeatedly (not so good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a toy model of random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_in = np.random.uniform(low=-1, high=+1, size=[batchsize, layerSizes[0]])\n",
    "y_target = np.random.uniform(low=-1, high=+1, size=[batchsize, layerSizes[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one training step:\n",
    "cost, y_out_result, Weights, Biases = train_net(y_in, y_target, .0001, Weights, Biases, numLayers, batchsize)  # returns cost function value\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eta = 0.001\n",
    "batches = 200\n",
    "\n",
    "costs = np.zeros(batches)  # array to store the costs\n",
    "\n",
    "for k in range(batches):\n",
    "    costs[k], y_out_result, Weights, Biases = train_net(y_in, y_target, eta, \n",
    "                                                        Weights, Biases, \n",
    "                                                        numLayers, batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this will show a very simple decrease, because\n",
    "# we are not yet stochastically sampling inputs\n",
    "# (it is always the SAME input! so the network\n",
    "# only becomes good for that input)\n",
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce random batches: randomly sample a function defined on a 2D square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a certain 2D function to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc(x0,x1):\n",
    "    r2 = x0**2+x1**2\n",
    "    return np.exp(-5*r2)*abs(x1+x0)\n",
    "\n",
    "xrange = np.linspace(-0.5, 0.5, 40)\n",
    "X0,X1 = np.meshgrid(xrange, xrange)\n",
    "plt.imshow(myFunc(X0,X1), interpolation='nearest', origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a NN to train and predict that function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up all the weights and biases\n",
    "\n",
    "NumLayers = 2  # does not count input-layer (but does count output)\n",
    "LayerSizes = [2,100,1]  # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "Weights = [np.random.uniform(low=-0.1, high=+0.1, size=[LayerSizes[j], LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "Biases = [np.zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# set up all the helper variables\n",
    "\n",
    "y_layer = [np.zeros(LayerSizes[j]) for j in range(NumLayers+1)]\n",
    "df_layer = [np.zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "dw_layer = [np.zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "db_layer = [np.zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# define the batchsize\n",
    "batchsize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a change: Set up rectified linear units (relu) \n",
    "# instead of sigmoid\n",
    "def net_f_df(z):  # calculate f(z) and f'(z)\n",
    "    val = z*(z>0)\n",
    "    return val, z>0  # return both f and f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_batchsize = np.shape(X0)[0]*np.shape(X0)[1]\n",
    "testsample = np.zeros([test_batchsize,2])\n",
    "testsample[:,0] = X0.flatten()\n",
    "testsample[:,1] = X1.flatten()\n",
    "\n",
    "def evaluate_current_state(samples, Weights, Biases, NumLayers, ax=None):\n",
    "    # try to evaluate the (randomly initialized) network\n",
    "    # on some area in the 2D plane\n",
    "    ax = plt.gca() if ax == None else ax\n",
    "\n",
    "    testoutput = apply_net_simple(samples, Weights, Biases, NumLayers)\n",
    "    myim = ax.imshow(np.reshape(testoutput,np.shape(X0)), origin='lower', interpolation='none')\n",
    "    ax.set_title(\"Current network prediction\")\n",
    "    \n",
    "evaluate_current_state(testsample, Weights, Biases, NumLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batchsize):\n",
    "    \n",
    "    inputs = np.random.uniform(low=-0.5, high=+0.5, size=[batchsize,2])\n",
    "    targets = np.zeros([batchsize,1])  # must have right dimensions\n",
    "    targets[:,0] = myFunc(inputs[:,0], inputs[:,1])\n",
    "    \n",
    "    return inputs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eta = 0.01 # learning rate\n",
    "nsteps = 100\n",
    "\n",
    "costs = np.zeros(nsteps)\n",
    "for j in range(nsteps):\n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(8,4))  # prepare figure\n",
    "    \n",
    "    # the crucial lines:\n",
    "    y_in, y_target = make_batch(batchsize)  # random samples (points in 2D)\n",
    "    costs[j], y_out_result, Weights, Biases = train_net(y_in,y_target,eta, Weights, Biases, NumLayers, batchsize)  # train network (one step, on this batch)\n",
    "    testoutput = apply_net_simple(testsample, Weights, Biases, NumLayers)  # check the new network output in the plane\n",
    "    \n",
    "    evaluate_current_state(testsample, Weights, Biases, NumLayers, ax[1])\n",
    "   \n",
    "    ax[0].plot(costs)\n",
    "    ax[0].set_title(\"Cost during training\")\n",
    "    ax[0].set_xlabel(\"number of batches\")\n",
    "    \n",
    "    plt.show()\n",
    "    sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train net to reproduce an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "# load the pixel image!\n",
    "face =imageio.imread('Smiley.png')\n",
    "pixel_image=np.transpose(face[:,:,0]) # have to transpose...\n",
    "pixel_image=pixel_image[:,::-1] # and flip... to get the right view!\n",
    "pixel_image-=pixel_image.min()\n",
    "pixel_image=(pixel_image.astype(dtype='float'))/pixel_image.max() # normalize between 0 and 1!\n",
    "Npixels=np.shape(pixel_image)[0] # assuming a square image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the function we want to have (desired outcome)\n",
    "# this picks the pixels from the image\n",
    "def myFunc(x0,x1):\n",
    "    global pixel_image, Npixels\n",
    "    # convert to integer coordinates (assuming input is 0..1)\n",
    "    x0int=(x0*Npixels*0.9999).astype(dtype='int')\n",
    "    x1int=(x1*Npixels*0.9999).astype(dtype='int')\n",
    "    return(pixel_image[x0int,x1int]) # extract color values at these pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check that this works:\n",
    "Npixels_Test=50 # do the test output on a low-res grid! (saves time)\n",
    "xrange=np.linspace(0,1,Npixels_Test)\n",
    "X0,X1=np.meshgrid(xrange,xrange)\n",
    "plt.imshow(myFunc(X0,X1),interpolation='nearest',origin='lower')\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_f_df(z):  # calculate f(z) and f'(z)\n",
    "    val = 1/(1+np.exp(-z))  # sigmoid\n",
    "    return val, np.exp(-z)*(val**2)  # return both f and f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up all the weights and biases\n",
    "\n",
    "NumLayers=3 # does not count input-layer (but does count output)\n",
    "LayerSizes=[2,150,100,1] # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "Weights=[5*(1./np.sqrt(LayerSizes[j]))*np.random.randn(LayerSizes[j],LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "Biases=[np.zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# set up all the helper variables\n",
    "\n",
    "y_layer=[np.zeros(LayerSizes[j]) for j in range(NumLayers+1)]\n",
    "df_layer=[np.zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "dw_layer=[np.zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "db_layer=[np.zeros(LayerSizes[j+1]) for j in range(NumLayers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 'test' batch that includes all the points on the image grid\n",
    "test_batchsize=np.shape(X0)[0]*np.shape(X0)[1]\n",
    "testsample=np.zeros([test_batchsize,2])\n",
    "testsample[:,0]=X0.flatten()\n",
    "testsample[:,1]=X1.flatten()\n",
    "\n",
    "evaluate_current_state(testsample, Weights, Biases, NumLayers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need about a million or so samples to get something reasonable looking\n",
    "i.e. at least resembling the shape\n",
    "\n",
    "EXERCISE: find better parameters, for weight initialization, for learning rate, for batchsize etc., to reduce the number of samples needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # set up all the weights and biases\n",
    "\n",
    "# NumLayers=3 # does not count input-layer (but does count output)\n",
    "# LayerSizes=[2,150,100,1] # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "# Weights=[5*(1./sqrt(LayerSizes[j]))*random.randn(LayerSizes[j],LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "# Biases=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# # set up all the helper variables\n",
    "\n",
    "# y_layer=[zeros(LayerSizes[j]) for j in range(NumLayers+1)]\n",
    "# df_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "# dw_layer=[zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "# db_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "samples_count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# test sample\n",
    "Npixels_Test=30 # do the test output on a low-res grid! (saves time)\n",
    "xrange=np.linspace(0,1,Npixels_Test)\n",
    "X0,X1=np.meshgrid(xrange,xrange)\n",
    "test_batchsize=np.shape(X0)[0]*np.shape(X0)[1]\n",
    "testsample=np.zeros([test_batchsize,2])\n",
    "testsample[:,0]=X0.flatten()\n",
    "testsample[:,1]=X1.flatten()\n",
    "\n",
    "# parameters\n",
    "eta=0.01\n",
    "nsteps=20000\n",
    "nskip_steps=100\n",
    "batchsize=1000\n",
    "\n",
    "\n",
    "costs=np.zeros(nsteps)\n",
    "\n",
    "for j in range(nsteps):\n",
    "    y_in,y_target=make_batch(batchsize)\n",
    "    costs[j], y_out_result, Weights, Biases=train_net(y_in,y_target,eta, Weights, Biases, NumLayers, batchsize)\n",
    "    samples_count+=batchsize\n",
    "    testoutput=apply_net_simple(testsample, Weights, Biases, NumLayers)  \n",
    "    if j%nskip_steps==0: # time to plot again!\n",
    "        clear_output(wait=True)\n",
    "        fig,ax = plt.subplots(ncols=2,nrows=1,figsize=(10,5))\n",
    "        img=ax[1].imshow(np.reshape(testoutput,np.shape(X0)),origin='lower',interpolation='nearest',vmin=0)\n",
    "        fig.colorbar(img,ax=ax[1])\n",
    "        ax[1].axis('off') # no axes\n",
    "        ax[0].plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "AllOldWeights=Weights # backup all weights\n",
    "OldWeights=Weights[-1] # especially of last layer, which we will modify below!\n",
    "\n",
    "samples_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the resulting network\n",
    "#savez_compressed(\"ImageCompression_Network_Smiley.npz\",LayerSizes,Weights,Biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# switch on only individual neurons of last hidden layer\n",
    "# and plot the resulting pictures in a big 10x10 array!\n",
    "#data=load(\"ImageCompression_Network_Smiley.npz\")\n",
    "#Weights=data['arr_1']\n",
    "\n",
    "Nrow=10\n",
    "BigImage=zeros([Nrow*Npixels_Test,Nrow*Npixels_Test])\n",
    "for which in range(100):\n",
    "    Weights[-1]=OldWeights.copy()\n",
    "    Weights[-1][0:which-1,:]=0\n",
    "    Weights[-1][which+1:-1,:]=0\n",
    "    testoutput=apply_net_simple(testsample)\n",
    "    row=int(which/Nrow)\n",
    "    column=which%Nrow\n",
    "    BigImage[Npixels_Test*row:Npixels_Test*(row+1),Npixels_Test*column:Npixels_Test*(column+1)]=reshape(testoutput,shape(X0))\n",
    "    #print(row,column)\n",
    "myim=plt.imshow(BigImage,origin='lower',interpolation='nearest',vmin=0.0,vmax=1.0)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
