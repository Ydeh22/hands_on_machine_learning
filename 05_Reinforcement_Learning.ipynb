{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=71605\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/deeplearning_course/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/deeplearning_course/blob/master/03_Reinforcement_Learning.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/deeplearning_course/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/deeplearning_course/blob/master/03_Reinforcement_Learning.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/deeplearning_course/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/deeplearning_course/master/03_Reinforcement_Learning.ipynb\"  target=\"_blank\"\n",
    "          download=\"03_Reinforcement_Learning\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/deeplearning_course/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{V}$. Reinforcement Learning\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/en/uab-official-masters-degrees-study-guides/) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "Tutorial 5\n",
    "\n",
    "This notebook shows how to:\n",
    "- solve a challenge with reinforcement learning\n",
    "- implement the Actor-Critic method\n",
    "- implement the Deep Deterministic Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports:-numpy-and-matplotlib-and-keras\" data-toc-modified-id=\"Imports:-numpy-and-matplotlib-and-keras-0\"><span class=\"toc-item-num\">0&nbsp;&nbsp;</span>Imports: numpy and matplotlib and keras</a></span></li><li><span><a href=\"#Actor-Critic-Method\" data-toc-modified-id=\"Actor-Critic-Method-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Actor Critic Method</a></span><ul class=\"toc-item\"><li><span><a href=\"#Configuration-parameters-for-the-whole-setup\" data-toc-modified-id=\"Configuration-parameters-for-the-whole-setup-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Configuration parameters for the whole setup</a></span></li><li><span><a href=\"#Neural-network-player\" data-toc-modified-id=\"Neural-network-player-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Neural network player</a></span></li><li><span><a href=\"#Reinforcement-training\" data-toc-modified-id=\"Reinforcement-training-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Reinforcement training</a></span></li><li><span><a href=\"#Playing\" data-toc-modified-id=\"Playing-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Playing</a></span></li></ul></li><li><span><a href=\"#Deep-Deterministic-Policy-Gradient-(DDPG)\" data-toc-modified-id=\"Deep-Deterministic-Policy-Gradient-(DDPG)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Deep Deterministic Policy Gradient (DDPG)</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-parameters\" data-toc-modified-id=\"General-parameters-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>General parameters</a></span></li><li><span><a href=\"#Useful-classes-(utils)\" data-toc-modified-id=\"Useful-classes-(utils)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Useful classes (utils)</a></span></li><li><span><a href=\"#NNs-definition\" data-toc-modified-id=\"NNs-definition-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>NNs definition</a></span></li><li><span><a href=\"#Policy-definition-and-training\" data-toc-modified-id=\"Policy-definition-and-training-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Policy definition and training</a></span></li><li><span><a href=\"#Playing\" data-toc-modified-id=\"Playing-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Playing</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports: numpy and matplotlib and keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install gym[all]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import gym\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Critic Method\n",
    "\n",
    "https://keras.io/examples/rl/actor_critic_cartpole/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration parameters for the whole setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 10000\n",
    "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
    "env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play: R, R, R, L, L, R, L, R, R, R, L, R, R, Done!\n",
      "The total reward is 13.00. Sorry, try again...\n"
     ]
    }
   ],
   "source": [
    "def play(environ, player=None, is_discrete=True, reward_to_win=200):\n",
    "\n",
    "    cum_reward = 0\n",
    "    rewards_list = []\n",
    "    state = environ.reset()\n",
    "\n",
    "    print(\"Let's play: \", flush=True, end='')\n",
    "    environ.render()\n",
    "    sleep(3)\n",
    "    \n",
    "    while True:\n",
    "               \n",
    "        if player:\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            action = player(state)\n",
    "            if is_discrete:\n",
    "                action = action[0]\n",
    "\n",
    "            if is_discrete:\n",
    "                action = np.random.choice(num_actions, p=np.squeeze(action))\n",
    "        else:\n",
    "            action = environ.action_space.sample()  # No player is random player\n",
    "        \n",
    "        action_str = ('L, ' if action == 0 else 'R, ' if is_discrete \n",
    "                      else (\"%.2f, \" % action.numpy()[0][0]))\n",
    "            \n",
    "        print(action_str, flush=True, end='')\n",
    "        state, reward, done, info = environ.step(action)\n",
    "        environ.render()\n",
    "        cum_reward += reward\n",
    "        rewards_list.append(reward)\n",
    "        if done:\n",
    "            print(\"Done!\")\n",
    "            final_mark = cum_reward if is_discrete else np.array(rewards_list[-40:]).mean()\n",
    "            win_lost = \"YOU WIN!\" if final_mark >= reward_to_win else \"Sorry, try again...\"\n",
    "            print(\"The total reward is %.2f.\" % final_mark, win_lost)\n",
    "            break\n",
    "\n",
    "play(env)  # Random player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          640         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            258         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            129         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,027\n",
      "Trainable params: 1,027\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "num_hidden = 128\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing episode 0... reward of 0.95\n",
      "Playing episode 10... reward of 14.37\n",
      "Playing episode 20... reward of 28.31\n",
      "Playing episode 30... reward of 26.46\n",
      "Playing episode 40... reward of 25.03\n",
      "Playing episode 50... reward of 38.82\n",
      "Playing episode 60... reward of 81.12\n",
      "Playing episode 70... reward of 126.88\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/gk/g7x0418s7s95lxtdnwqt0xs80000gn/T/ipykernel_16948/224201735.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     82\u001B[0m         \u001B[0;31m# Backpropagation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[0mloss_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mactor_losses\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcritic_losses\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 84\u001B[0;31m         \u001B[0mgrads\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss_value\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainable_variables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     85\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_gradients\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgrads\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainable_variables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001B[0m in \u001B[0;36mgradient\u001B[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001B[0m\n\u001B[1;32m   1082\u001B[0m                           for x in nest.flatten(output_gradients)]\n\u001B[1;32m   1083\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1084\u001B[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001B[0m\u001B[1;32m   1085\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tape\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1086\u001B[0m         \u001B[0mflat_targets\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py\u001B[0m in \u001B[0;36mimperative_grad\u001B[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001B[0m\n\u001B[1;32m     69\u001B[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001B[1;32m     70\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 71\u001B[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001B[0m\u001B[1;32m     72\u001B[0m       \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tape\u001B[0m\u001B[0;34m,\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m       \u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001B[0m in \u001B[0;36m_gradient_function\u001B[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001B[0m\n\u001B[1;32m    157\u001B[0m       \u001B[0mgradient_name_scope\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mforward_pass_name_scope\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"/\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    158\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgradient_name_scope\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 159\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mgrad_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmock_op\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mout_grads\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    160\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    161\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mgrad_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmock_op\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mout_grads\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py\u001B[0m in \u001B[0;36m_SelectGradV2\u001B[0;34m(op, grad)\u001B[0m\n\u001B[1;32m   1681\u001B[0m   \u001B[0mgx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1682\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1683\u001B[0;31m   \u001B[0mgy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwhere_v2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mzeros\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1684\u001B[0m   \u001B[0my_shape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1685\u001B[0m   \u001B[0;31m# Reduce away broadcasted leading dims.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mop_dispatch_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1094\u001B[0m       \u001B[0;31m# Fallback dispatch system (dispatch v1):\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1095\u001B[0m       \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1096\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mdispatch_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1097\u001B[0m       \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1098\u001B[0m         \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001B[0m in \u001B[0;36mwhere_v2\u001B[0;34m(condition, x, y, name)\u001B[0m\n\u001B[1;32m   4786\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mgen_array_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwhere\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcondition\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcondition\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4787\u001B[0m   \u001B[0;32melif\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0my\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4788\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect_v2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcondition\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcondition\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4789\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4790\u001B[0m     \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"x and y must both be non-None or both be None.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36mselect_v2\u001B[0;34m(condition, t, e, name)\u001B[0m\n\u001B[1;32m   9361\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   9362\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 9363\u001B[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[1;32m   9364\u001B[0m         _ctx, \"SelectV2\", name, condition, t, e)\n\u001B[1;32m   9365\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:  # Run until solved\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    do_show = episode_count % 10 == 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        if do_show:\n",
    "            print(\"Playing episode %d...\" % episode_count, flush=True, end='')\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            \"\"\" Loop over one episode \"\"\"\n",
    "\n",
    "            if do_show:\n",
    "                env.render()\n",
    "            \n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # Predict action probabilities and estimated future rewards\n",
    "            # from environment state\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "            \n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "#                 print(\"Episode %d done\" % episode_count)\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "                    )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if do_show:\n",
    "        print(\" reward of %.2f\" % running_reward, flush=True)\n",
    "\n",
    "    if running_reward > 195:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play: L, R, L, R, R, R, R, L, L, L, R, R, L, L, L, R, R, R, L, R, L, L, R, L, R, L, L, L, R, L, R, L, L, R, R, R, L, R, L, L, R, L, R, L, R, R, L, R, L, R, R, L, R, L, L, L, R, L, L, R, R, R, L, R, L, R, R, L, R, L, R, L, R, R, L, L, R, L, R, R, R, L, R, L, R, L, R, L, L, R, L, R, R, L, R, R, L, R, L, L, R, R, R, R, L, L, L, R, L, R, L, L, R, R, L, R, L, L, R, L, R, L, L, R, L, R, L, R, R, L, L, L, R, L, R, L, R, L, R, L, R, R, L, L, L, L, R, L, L, L, R, R, L, L, L, L, R, L, L, L, L, R, R, L, L, R, L, R, R, R, L, R, R, L, R, L, R, R, R, L, R, L, L, R, L, R, Done!\n",
      "The total reward is 186. Sorry, try again...\n"
     ]
    }
   ],
   "source": [
    "play(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "https://keras.io/examples/rl/ddpg_pendulum/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    }
   ],
   "source": [
    "problem = \"Pendulum-v1\"\n",
    "env2 = gym.make(problem)\n",
    "\n",
    "num_states = env2.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env2.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env2.action_space.high[0]\n",
    "lower_bound = env2.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play: R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, Done!\n",
      "The total reward is -999. Sorry, try again...\n"
     ]
    }
   ],
   "source": [
    "play(env2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful classes (utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNs definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1364.4024310365621\n",
      "Episode * 1 * Avg Reward is ==> -1353.0310244714447\n",
      "Episode * 2 * Avg Reward is ==> -1362.2955801719797\n",
      "Episode * 3 * Avg Reward is ==> -1395.1200890479408\n",
      "Episode * 4 * Avg Reward is ==> -1424.710933336597\n",
      "Episode * 5 * Avg Reward is ==> -1395.1406532937683\n",
      "Episode * 6 * Avg Reward is ==> -1354.513781808074\n",
      "Episode * 7 * Avg Reward is ==> -1284.9956431724515\n",
      "Episode * 8 * Avg Reward is ==> -1226.740104760154\n",
      "Episode * 9 * Avg Reward is ==> -1117.998949485124\n",
      "Episode * 10 * Avg Reward is ==> -1028.1361325209098\n",
      "Episode * 11 * Avg Reward is ==> -963.7622207874406\n",
      "Episode * 12 * Avg Reward is ==> -900.2305951967496\n",
      "Episode * 13 * Avg Reward is ==> -854.4256879152889\n",
      "Episode * 14 * Avg Reward is ==> -813.7950088060511\n",
      "Episode * 15 * Avg Reward is ==> -771.0515704037023\n",
      "Episode * 16 * Avg Reward is ==> -725.7720871672288\n",
      "Episode * 17 * Avg Reward is ==> -685.5356460700445\n",
      "Episode * 18 * Avg Reward is ==> -662.3551922636465\n",
      "Episode * 19 * Avg Reward is ==> -635.7646134061175\n",
      "Episode * 20 * Avg Reward is ==> -617.5958098114863\n",
      "Episode * 21 * Avg Reward is ==> -589.5725554025134\n",
      "Episode * 22 * Avg Reward is ==> -574.2206340983489\n",
      "Episode * 23 * Avg Reward is ==> -555.4397543462707\n",
      "Episode * 24 * Avg Reward is ==> -542.9796711511642\n",
      "Episode * 25 * Avg Reward is ==> -526.7970288196507\n",
      "Episode * 26 * Avg Reward is ==> -511.9646305277507\n",
      "Episode * 27 * Avg Reward is ==> -498.05970392149294\n",
      "Episode * 28 * Avg Reward is ==> -480.94676675095536\n",
      "Episode * 29 * Avg Reward is ==> -473.2265506730832\n",
      "Episode * 30 * Avg Reward is ==> -468.14753092751147\n",
      "Episode * 31 * Avg Reward is ==> -457.40252750082703\n",
      "Episode * 32 * Avg Reward is ==> -447.11768803828045\n",
      "Episode * 33 * Avg Reward is ==> -444.3087642480273\n",
      "Episode * 34 * Avg Reward is ==> -435.2954419290767\n",
      "Episode * 35 * Avg Reward is ==> -430.2334689372638\n",
      "Episode * 36 * Avg Reward is ==> -425.47892731227194\n",
      "Episode * 37 * Avg Reward is ==> -417.5927366942779\n",
      "Episode * 38 * Avg Reward is ==> -410.02213304523156\n",
      "Episode * 39 * Avg Reward is ==> -402.94584837333446\n",
      "Episode * 40 * Avg Reward is ==> -371.86867281697243\n",
      "Episode * 41 * Avg Reward is ==> -346.73850881218476\n",
      "Episode * 42 * Avg Reward is ==> -315.3731077769356\n",
      "Episode * 43 * Avg Reward is ==> -281.2815198816375\n",
      "Episode * 44 * Avg Reward is ==> -252.03214607873605\n",
      "Episode * 45 * Avg Reward is ==> -224.03654474000376\n",
      "Episode * 46 * Avg Reward is ==> -199.46713018805073\n",
      "Episode * 47 * Avg Reward is ==> -188.1779682906134\n",
      "Episode * 48 * Avg Reward is ==> -169.186236786755\n",
      "Episode * 49 * Avg Reward is ==> -171.84423277580999\n",
      "Episode * 50 * Avg Reward is ==> -171.50009658080398\n",
      "Episode * 51 * Avg Reward is ==> -165.1317934102857\n",
      "Episode * 52 * Avg Reward is ==> -164.730871351942\n",
      "Episode * 53 * Avg Reward is ==> -161.34393521502884\n",
      "Episode * 54 * Avg Reward is ==> -164.65924855771473\n",
      "Episode * 55 * Avg Reward is ==> -170.76693497194432\n",
      "Episode * 56 * Avg Reward is ==> -170.90890632741508\n",
      "Episode * 57 * Avg Reward is ==> -180.30841294470832\n",
      "Episode * 58 * Avg Reward is ==> -183.68615301102707\n",
      "Episode * 59 * Avg Reward is ==> -183.67043205731898\n",
      "Episode * 60 * Avg Reward is ==> -186.0590066673159\n",
      "Episode * 61 * Avg Reward is ==> -189.11895047598279\n",
      "Episode * 62 * Avg Reward is ==> -192.46063641124488\n",
      "Episode * 63 * Avg Reward is ==> -192.4095324778168\n",
      "Episode * 64 * Avg Reward is ==> -189.54878237326025\n",
      "Episode * 65 * Avg Reward is ==> -189.75297293671275\n",
      "Episode * 66 * Avg Reward is ==> -195.88057700312143\n",
      "Episode * 67 * Avg Reward is ==> -198.9063657773551\n",
      "Episode * 68 * Avg Reward is ==> -202.02592476658316\n",
      "Episode * 69 * Avg Reward is ==> -201.64810148912056\n",
      "Episode * 70 * Avg Reward is ==> -199.93105972531197\n",
      "Episode * 71 * Avg Reward is ==> -202.73132396257492\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/gk/g7x0418s7s95lxtdnwqt0xs80000gn/T/ipykernel_16948/28784840.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[0mbuffer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlearn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m         \u001B[0mupdate_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtarget_actor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvariables\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mactor_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvariables\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtau\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m         \u001B[0mupdate_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtarget_critic\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvariables\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcritic_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvariables\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtau\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    908\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    909\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 910\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    911\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    912\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    947\u001B[0m       \u001B[0;31m# In this case we have not created variables on the first call. So we can\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    948\u001B[0m       \u001B[0;31m# run the first trace but we should fail if variables are created.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 949\u001B[0;31m       \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    950\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_created_variables\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    951\u001B[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3128\u001B[0m       (graph_function,\n\u001B[1;32m   3129\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[0;32m-> 3130\u001B[0;31m     return graph_function._call_flat(\n\u001B[0m\u001B[1;32m   3131\u001B[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[1;32m   3132\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1957\u001B[0m         and executing_eagerly):\n\u001B[1;32m   1958\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1959\u001B[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[1;32m   1960\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[1;32m   1961\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    596\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    597\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 598\u001B[0;31m           outputs = execute.execute(\n\u001B[0m\u001B[1;32m    599\u001B[0m               \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    600\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/photonicMaster/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     56\u001B[0m   \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[1;32m     59\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[1;32m     60\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env2.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        if ep % 5 == 0:\n",
    "            env2.render()\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, info = env2.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * final_mark is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.plot(ep_reward_list)\n",
    "plt.legend([\"Averaged last 40\", \"Single episode\"])\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play: -2.00, -2.00, -2.00, -2.00, -2.00, -2.00, -2.00, -2.00, -2.00, -2.00, -1.92, -0.18, -1.66, -0.08, -1.99, -1.33, 1.35, -1.92, 1.95, 0.83, -0.99, 1.40, -1.98, 0.41, -1.95, 1.10, -2.00, -0.69, -0.61, -1.03, 0.72, -2.00, -0.76, -0.32, -1.69, 0.99, -1.99, 0.40, -1.98, 0.65, -1.99, 0.15, -1.96, 0.89, -1.99, 0.01, -1.92, 0.92, -1.99, 0.32, -1.97, 0.79, -1.99, -0.10, -1.88, 0.94, -1.98, 0.48, -1.98, 0.53, -1.98, 0.44, -1.98, 0.60, -1.99, 0.29, -1.97, 0.81, -1.99, -0.15, -1.85, 0.95, -1.98, 0.51, -1.98, 0.48, -1.98, 0.53, -1.99, 0.43, -1.98, 0.61, -1.99, 0.26, -1.97, 0.85, -1.99, -0.20, -1.82, 0.95, -1.98, 0.53, -1.98, 0.44, -1.98, 0.60, -1.99, 0.29, -1.97, 0.82, -1.99, -0.15, -1.85, 0.94, -1.98, 0.51, -1.98, 0.48, -1.98, 0.54, -1.99, 0.42, -1.98, 0.63, -1.99, 0.22, -1.97, 0.86, -1.99, -0.13, -1.86, 0.94, -1.98, 0.50, -1.98, 0.49, -1.98, 0.50, -1.98, 0.49, -1.98, 0.51, -1.99, 0.47, -1.98, 0.54, -1.99, 0.41, -1.98, 0.65, -1.99, 0.17, -1.96, 0.88, -1.99, -0.01, -1.91, 0.92, -1.99, 0.37, -1.98, 0.73, -1.99, -0.02, -1.91, 0.92, -1.99, 0.38, -1.98, 0.70, -1.99, 0.06, -1.93, 0.91, -1.99, 0.27, -1.97, 0.84, -1.99, -0.16, -1.84, 0.94, -1.98, 0.53, -1.98, 0.45, -1.98, 0.59, -1.99, 0.32, -1.97, 0.79, -1.99, -0.11, -1.87, 0.94, -1.98, 0.49, -1.98, 0.52, -1.98, 0.46, -1.98, 0.56, -1.99, 0.37, -1.98, 0.72, -1.99, -0.01, Done!\n",
      "The total reward is -0.02. Sorry, try again...\n"
     ]
    }
   ],
   "source": [
    "play(env2, target_actor, False, -0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}